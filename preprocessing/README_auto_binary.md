

# README

## Overview

This repository implements a **learning-based image preprocessing and segmentation pipeline** for Two-Photon Laser Scanning Microscopy (2PLSM) images.  
The goal is to automatically generate standardized **binary masks of dendritic structures** from intensity images, replacing traditional manual or rule-based segmentation procedures.

The pipeline is designed as a **data-driven alternative to classical thresholding-based preprocessing**, leveraging a lightweight U-Net model trained on paired intensity–binary images.  
The resulting binary masks can be directly used for downstream morphological or shape-based analysis.



## Processing Logic

The preprocessing and segmentation pipeline follows the steps below:

1. **Input loading and normalization**
   
   - Intensity images are loaded in grayscale format.
   
   - Pixel intensities are normalized to the range ([0, 1]).

2. **Spatial standardization**
   
   - All images are resized to a fixed resolution of **250 × 250 pixels**.
   
   - Corresponding binary masks are resized using nearest-neighbor interpolation to preserve label integrity.

3. **Dataset pairing**
   
   - Each intensity image is paired with its corresponding binary annotation based on a shared numeric identifier in the filename.
   
   - Only successfully paired samples are included in training and evaluation.

4. **Data augmentation (training only)**
   
   - To improve robustness to spatial and illumination variability, the following augmentations are applied:
     
     - Random horizontal and vertical flips
     
     - Random 90-degree rotations
     
     - Mild intensity scaling and offset (applied only to intensity images)
   
   - Binary masks undergo identical geometric transformations.

5. **Segmentation using Tiny U-Net**
   
   - A lightweight U-Net architecture is trained to perform pixel-wise binary segmentation.
   
   - The network outputs a logit map representing foreground likelihood for each pixel.

6. **Loss function and optimization**
   
   - Training uses a **composite loss** consisting of:
     
     - Binary Cross-Entropy (BCE) loss
     
     - Dice loss
   
   - This combination balances pixel-wise accuracy and region-level overlap.

7. **Probability thresholding**
   
   - Model outputs are converted to probability maps using a sigmoid activation.
   
   - Binary masks are generated by applying a fixed probability threshold.

8. **Threshold scanning and selection**
   
   - A range of candidate thresholds is evaluated on the validation set.
   
   - The threshold that maximizes the **Dice coefficient** is selected and used for final inference.

9. **Output generation**
   
   - The final output is a binary mask of size **250 × 250**, representing the segmented dendritic structure.



## Model Architecture

The segmentation model is a compact U-Net variant consisting of:

- Encoder:
  
  - Three convolutional blocks with downsampling

- Decoder:
  
  - Two upsampling stages with skip connections

- Output:
  
  - A single-channel logit map corresponding to foreground probability

The architecture is intentionally lightweight to allow training on CPU and to reduce overfitting given the limited dataset size.



## Evaluation Metrics

Model performance is evaluated using:

- **Dice coefficient** (primary metric)

- Binary Cross-Entropy + Dice loss (training objective)

Dice score is reported for training, validation, and test sets.  
Threshold selection is performed based exclusively on validation Dice performance.



## Input Requirements

To achieve reliable segmentation performance, the following assumptions should hold:

- The target dendritic structure is approximately centered in the image.

- Each image contains a single dominant foreground structure.

- Intensity images and binary masks are correctly paired and aligned.

- Imaging conditions are broadly consistent with those seen during training.

Violation of these assumptions may degrade segmentation quality.



## Important Limitations

- This pipeline is **not fully unsupervised**; it relies on paired intensity–binary annotations for training.

- Segmentation performance depends on the quality and consistency of the provided annotations.

- While threshold selection is automated via validation scanning, the threshold remains a global parameter and may not generalize optimally to drastically different datasets.

- Extremely low-contrast or highly noisy images may still require manual inspection.



## Intended Use

This pipeline is intended for:

- Automated binary mask generation for downstream morphological analysis

- Replacing manual or rule-based preprocessing in exploratory studies

- Demonstrating the feasibility of learning-based preprocessing for 2PLSM data

It is **not intended as a fully general-purpose segmentation framework** without dataset-specific validation.



## Reproducibility

- All experiments use fixed random seeds to ensure reproducibility.

- Training can be resumed from saved checkpoints without re-running the full pipeline.

- Model checkpoints and training histories are saved automatically.


