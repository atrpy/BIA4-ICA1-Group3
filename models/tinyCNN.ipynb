{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:52:29.089704Z",
     "start_time": "2025-12-10T10:52:29.053797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TinySpineNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(TinySpineNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        # <-- use a unified self.fc (Sequential) -->\n",
    "        self.fc = nn.Sequential(\n",
    "                    nn.Linear(256, 192),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.35),\n",
    "                    nn.Linear(192, 96),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(96, num_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ],
   "id": "9ed7b87cebc0daa3",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:52:29.991231Z",
     "start_time": "2025-12-10T10:52:29.801154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_file = 'Dataset_Labels.xlsx'\n",
    "df = pd.read_excel(label_file)\n",
    "df.columns = ['Spine_Name', 'Spine_Label']\n",
    "\n",
    "# Classify by category proportion to ensure fair comparison\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['Spine_Label']\n",
    ")\n",
    "\n",
    "train_df.to_csv(\"spine_train_split.csv\", index=False)\n",
    "test_df.to_csv(\"spine_test_split.csv\", index=False)\n",
    "\n",
    "print(\"Saved spine_train_split.csv and spine_test_split.csv\")\n",
    "print(\"Train size =\", len(train_df), \" Test size =\", len(test_df))\n",
    "print(train_df['Spine_Label'].value_counts())\n",
    "print(test_df['Spine_Label'].value_counts())\n"
   ],
   "id": "930d4f7d722d1d3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved spine_train_split.csv and spine_test_split.csv\n",
      "Train size = 364  Test size = 92\n",
      "Spine_Label\n",
      "Mushroom    230\n",
      "Stubby       90\n",
      "Thin         44\n",
      "Name: count, dtype: int64\n",
      "Spine_Label\n",
      "Mushroom    58\n",
      "Stubby      23\n",
      "Thin        11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:52:34.982779Z",
     "start_time": "2025-12-10T10:52:34.925200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import visdom\n",
    "from PIL import Image\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# ==========================\n",
    "# Hyperparameters\n",
    "# ==========================\n",
    "BATCH_SIZE = 32               # batch size\n",
    "                # total training epochs\n",
    "save_path = \"./Spine_tinyCNN.pth\"  # model save path\n",
    "\n",
    "# ==========================\n",
    "# Paths\n",
    "# ==========================\n",
    "img_dir = 'Dataset Binary'          # image folder\n",
    "label_file = 'Dataset_Labels.xlsx'  # label file\n",
    "train_df = pd.read_csv(\"spine_train_split.csv\")\n",
    "val_df   = pd.read_csv(\"spine_test_split.csv\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Data Augmentation\n",
    "# ==========================\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((250, 250)),\n",
    "        transforms.RandomRotation(10),                      # mild rotation\n",
    "        transforms.RandomResizedCrop(250, scale=(0.9, 1.0)),# mild scale change\n",
    "        transforms.RandomHorizontalFlip(p=0.5),             # useful if bilateral symmetry holds\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((250, 250)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "# ==========================\n",
    "# Custom Dataset\n",
    "# ==========================\n",
    "class BinarySpineDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # String â†’ numeric label mapping\n",
    "        self.label_map = {\n",
    "            \"Mushroom\": 0,\n",
    "            \"Stubby\": 1,\n",
    "            \"Thin\": 2\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row['Spine_Name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.label_map[row['Spine_Label']]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# ==========================\n",
    "# Train / Val Split\n",
    "# ==========================\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_dataset = BinarySpineDataset(train_df, img_dir, transform=data_transform[\"train\"])\n",
    "val_dataset   = BinarySpineDataset(val_df, img_dir, transform=data_transform[\"val\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ==========================\n",
    "# Device\n",
    "# ==========================\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ],
   "id": "4b8871fb8fdc0510",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:52:59.285401Z",
     "start_time": "2025-12-10T10:52:59.280611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import visdom\n",
    "from PIL import Image\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch import optim\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 100\n",
    "save_path = \"./Spine_tinyCNN.pth\""
   ],
   "id": "5505cd490ba4ccc0",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:17:47.606552Z",
     "start_time": "2025-12-10T12:18:45.764315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import visdom\n",
    "\n",
    "viz = visdom.Visdom(env=\"spine_exp\")\n",
    "\n",
    "# ====================================\n",
    "# test function\n",
    "# ====================================\n",
    "def evalute(model, loader):\n",
    "    correct = 0\n",
    "    total = len(loader.dataset)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == y).float().sum().item()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# create model\n",
    "# ====================================\n",
    "net = TinySpineNet(num_classes=3).to(device)\n",
    "\n",
    "# If you wish to continue the training, please remove the following notes\n",
    "checkpoint = torch.load(save_path)\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "best_acc = checkpoint['best_acc']\n",
    "start_epoch = checkpoint['epoch']\n",
    "\n",
    "# ====================================\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0008)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=2\n",
    ")\n",
    "loss_function = CrossEntropyLoss()\n",
    "\n",
    "best_acc = 0.0\n",
    "best_epoch = 0\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Training cycle\n",
    "# ====================================\n",
    "for epoch in range(start_epoch, EPOCH):\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_loader, start=0):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # train accuracy\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        train_correct = (pred == labels).float().sum().item()\n",
    "        train_acc = train_correct / labels.size(0)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # progress bar\n",
    "        rate = (step + 1) / len(train_loader)\n",
    "        a = \"*\" * int(rate * 50)\n",
    "        b = \".\" * int((1 - rate) * 50)\n",
    "        print(\n",
    "            \"\\repoch: {} train loss: {:^3.0f}%[{}->{}]{:.3f}  train_acc:{:.3f}\".format(\n",
    "                epoch + 1, int(rate * 100), a, b, loss, train_acc\n",
    "            ),\n",
    "            end=\"\", flush=True\n",
    "        )\n",
    "\n",
    "        viz.line([train_acc], [global_step], win='train_acc', update='append')\n",
    "        viz.line([loss.item()], [global_step], win='loss', update='append')\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    # ===============================\n",
    "    # test each epoch\n",
    "    # ===============================\n",
    "    test_acc = evalute(net, val_loader)\n",
    "    print(\"  epoch{} test acc:{}\".format(epoch + 1, test_acc))\n",
    "\n",
    "    # visdom update\n",
    "    viz.line([test_acc], [global_step], win='test_acc', update='append')\n",
    "\n",
    "    # ===============================\n",
    "    # save best model\n",
    "    # ===============================\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': best_epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_acc': best_acc\n",
    "        }, save_path)\n",
    "\n",
    "        print(f\"ðŸ”¥ Saved best model at epoch {best_epoch}, acc={best_acc:.4f}\")\n",
    "\n",
    "print(\"Finish !\")\n",
    "print(\"Best epoch:\", best_epoch, \" Best Acc:\", best_acc)\n"
   ],
   "id": "dbdbf9615d9d824c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "/var/folders/4n/_x3hbgpx0dz1zn2b0yt3w1_m0000gn/T/ipykernel_1445/1533478980.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train loss: 100%[**************************************************->]0.335  train_acc:0.917  epoch1 test acc:0.8043478260869565\n",
      "ðŸ”¥ Saved best model at epoch 1, acc=0.8043\n",
      "epoch: 2 train loss: 100%[**************************************************->]0.254  train_acc:0.917  epoch2 test acc:0.8804347826086957\n",
      "ðŸ”¥ Saved best model at epoch 2, acc=0.8804\n",
      "epoch: 3 train loss: 100%[**************************************************->]0.473  train_acc:0.833  epoch3 test acc:0.782608695652174\n",
      "epoch: 4 train loss: 100%[**************************************************->]0.051  train_acc:1.000  epoch4 test acc:0.8260869565217391\n",
      "epoch: 5 train loss: 100%[**************************************************->]0.379  train_acc:0.833  epoch5 test acc:0.9021739130434783\n",
      "ðŸ”¥ Saved best model at epoch 5, acc=0.9022\n",
      "epoch: 6 train loss: 100%[**************************************************->]0.263  train_acc:0.833  epoch6 test acc:0.8260869565217391\n",
      "epoch: 7 train loss: 100%[**************************************************->]0.135  train_acc:1.000  epoch7 test acc:0.8152173913043478\n",
      "epoch: 8 train loss: 100%[**************************************************->]0.228  train_acc:0.917  epoch8 test acc:0.8804347826086957\n",
      "epoch: 9 train loss: 100%[**************************************************->]0.110  train_acc:1.000  epoch9 test acc:0.7934782608695652\n",
      "epoch: 10 train loss: 100%[**************************************************->]0.067  train_acc:1.000  epoch10 test acc:0.6195652173913043\n",
      "epoch: 11 train loss: 100%[**************************************************->]0.095  train_acc:1.000  epoch11 test acc:0.6847826086956522\n",
      "epoch: 12 train loss: 100%[**************************************************->]0.154  train_acc:0.917  epoch12 test acc:0.8804347826086957\n",
      "epoch: 13 train loss: 100%[**************************************************->]0.213  train_acc:0.917  epoch13 test acc:0.7391304347826086\n",
      "epoch: 14 train loss: 100%[**************************************************->]0.192  train_acc:0.917  epoch14 test acc:0.9130434782608695\n",
      "ðŸ”¥ Saved best model at epoch 14, acc=0.9130\n",
      "epoch: 15 train loss: 100%[**************************************************->]0.101  train_acc:1.000  epoch15 test acc:0.8913043478260869\n",
      "epoch: 16 train loss: 100%[**************************************************->]0.474  train_acc:0.917  epoch16 test acc:0.9130434782608695\n",
      "epoch: 17 train loss: 100%[**************************************************->]0.089  train_acc:0.917  epoch17 test acc:0.8152173913043478\n",
      "epoch: 18 train loss: 100%[**************************************************->]0.098  train_acc:1.000  epoch18 test acc:0.8586956521739131\n",
      "epoch: 19 train loss: 100%[**************************************************->]0.614  train_acc:0.833  epoch19 test acc:0.7934782608695652\n",
      "epoch: 20 train loss: 100%[**************************************************->]0.040  train_acc:1.000  epoch20 test acc:0.6630434782608695\n",
      "epoch: 21 train loss: 100%[**************************************************->]0.106  train_acc:1.000  epoch21 test acc:0.8478260869565217\n",
      "epoch: 22 train loss: 100%[**************************************************->]0.053  train_acc:1.000  epoch22 test acc:0.6413043478260869\n",
      "epoch: 23 train loss: 100%[**************************************************->]0.142  train_acc:0.917  epoch23 test acc:0.9130434782608695\n",
      "epoch: 24 train loss: 100%[**************************************************->]0.072  train_acc:1.000  epoch24 test acc:0.8478260869565217\n",
      "epoch: 25 train loss: 100%[**************************************************->]0.358  train_acc:0.917  epoch25 test acc:0.8043478260869565\n",
      "epoch: 26 train loss: 100%[**************************************************->]0.249  train_acc:0.833  epoch26 test acc:0.8586956521739131\n",
      "epoch: 27 train loss: 100%[**************************************************->]0.635  train_acc:0.750  epoch27 test acc:0.33695652173913043\n",
      "epoch: 28 train loss: 100%[**************************************************->]0.090  train_acc:1.000  epoch28 test acc:0.8369565217391305\n",
      "epoch: 29 train loss: 100%[**************************************************->]0.309  train_acc:0.833  epoch29 test acc:0.7717391304347826\n",
      "epoch: 30 train loss: 100%[**************************************************->]0.264  train_acc:0.917  epoch30 test acc:0.8804347826086957\n",
      "epoch: 31 train loss: 100%[**************************************************->]0.051  train_acc:1.000  epoch31 test acc:0.8804347826086957\n",
      "epoch: 32 train loss: 100%[**************************************************->]0.357  train_acc:0.833  epoch32 test acc:0.8913043478260869\n",
      "epoch: 33 train loss: 100%[**************************************************->]0.136  train_acc:0.917  epoch33 test acc:0.8369565217391305\n",
      "epoch: 34 train loss: 100%[**************************************************->]0.116  train_acc:1.000  epoch34 test acc:0.6630434782608695\n",
      "epoch: 35 train loss: 100%[**************************************************->]0.147  train_acc:0.917  epoch35 test acc:0.7391304347826086\n",
      "epoch: 36 train loss: 100%[**************************************************->]0.103  train_acc:0.917  epoch36 test acc:0.8913043478260869\n",
      "epoch: 37 train loss: 100%[**************************************************->]0.055  train_acc:1.000  epoch37 test acc:0.9130434782608695\n",
      "epoch: 38 train loss: 100%[**************************************************->]0.112  train_acc:1.000  epoch38 test acc:0.9021739130434783\n",
      "epoch: 39 train loss: 100%[**************************************************->]0.211  train_acc:0.917  epoch39 test acc:0.9021739130434783\n",
      "epoch: 40 train loss: 100%[**************************************************->]0.294  train_acc:0.833  epoch40 test acc:0.9130434782608695\n",
      "epoch: 41 train loss: 100%[**************************************************->]0.104  train_acc:1.000  epoch41 test acc:0.8478260869565217\n",
      "epoch: 42 train loss: 100%[**************************************************->]0.239  train_acc:0.917  epoch42 test acc:0.4891304347826087\n",
      "epoch: 43 train loss: 100%[**************************************************->]0.073  train_acc:1.000  epoch43 test acc:0.9130434782608695\n",
      "epoch: 44 train loss: 100%[**************************************************->]0.617  train_acc:0.833  epoch44 test acc:0.6956521739130435\n",
      "epoch: 45 train loss: 100%[**************************************************->]0.174  train_acc:0.917  epoch45 test acc:0.8586956521739131\n",
      "epoch: 46 train loss: 100%[**************************************************->]0.044  train_acc:1.000  epoch46 test acc:0.8695652173913043\n",
      "epoch: 47 train loss: 100%[**************************************************->]0.058  train_acc:1.000  epoch47 test acc:0.782608695652174\n",
      "epoch: 48 train loss: 100%[**************************************************->]0.119  train_acc:0.917  epoch48 test acc:0.9021739130434783\n",
      "epoch: 49 train loss: 100%[**************************************************->]0.080  train_acc:1.000  epoch49 test acc:0.9239130434782609\n",
      "ðŸ”¥ Saved best model at epoch 49, acc=0.9239\n",
      "epoch: 50 train loss: 100%[**************************************************->]0.024  train_acc:1.000  epoch50 test acc:0.8804347826086957\n",
      "epoch: 51 train loss: 100%[**************************************************->]0.270  train_acc:0.917  epoch51 test acc:0.9239130434782609\n",
      "epoch: 52 train loss: 100%[**************************************************->]0.183  train_acc:0.917  epoch52 test acc:0.7608695652173914\n",
      "epoch: 53 train loss: 100%[**************************************************->]0.340  train_acc:0.917  epoch53 test acc:0.9021739130434783\n",
      "epoch: 54 train loss: 100%[**************************************************->]0.161  train_acc:1.000  epoch54 test acc:0.8478260869565217\n",
      "epoch: 55 train loss: 100%[**************************************************->]0.137  train_acc:0.917  epoch55 test acc:0.9239130434782609\n",
      "epoch: 56 train loss: 100%[**************************************************->]0.054  train_acc:1.000  epoch56 test acc:0.6847826086956522\n",
      "epoch: 57 train loss: 100%[**************************************************->]0.644  train_acc:0.833  epoch57 test acc:0.9130434782608695\n",
      "epoch: 58 train loss: 100%[**************************************************->]0.572  train_acc:0.750  epoch58 test acc:0.8804347826086957\n",
      "epoch: 59 train loss: 100%[**************************************************->]0.088  train_acc:1.000  epoch59 test acc:0.7717391304347826\n",
      "epoch: 60 train loss: 100%[**************************************************->]0.203  train_acc:1.000  epoch60 test acc:0.9239130434782609\n",
      "epoch: 61 train loss: 100%[**************************************************->]0.496  train_acc:0.750  epoch61 test acc:0.717391304347826\n",
      "epoch: 62 train loss: 100%[**************************************************->]0.208  train_acc:0.917  epoch62 test acc:0.9347826086956522\n",
      "ðŸ”¥ Saved best model at epoch 62, acc=0.9348\n",
      "epoch: 63 train loss: 100%[**************************************************->]0.020  train_acc:1.000  epoch63 test acc:0.8913043478260869\n",
      "epoch: 64 train loss: 100%[**************************************************->]0.075  train_acc:1.000  epoch64 test acc:0.782608695652174\n",
      "epoch: 65 train loss: 100%[**************************************************->]0.049  train_acc:1.000  epoch65 test acc:0.7282608695652174\n",
      "epoch: 66 train loss: 100%[**************************************************->]0.405  train_acc:0.917  epoch66 test acc:0.9347826086956522\n",
      "epoch: 67 train loss: 100%[**************************************************->]1.240  train_acc:0.583  epoch67 test acc:0.6956521739130435\n",
      "epoch: 68 train loss: 100%[**************************************************->]0.197  train_acc:0.917  epoch68 test acc:0.9239130434782609\n",
      "epoch: 69 train loss: 100%[**************************************************->]0.136  train_acc:1.000  epoch69 test acc:0.8695652173913043\n",
      "epoch: 70 train loss: 100%[**************************************************->]0.517  train_acc:0.833  epoch70 test acc:0.9130434782608695\n",
      "epoch: 71 train loss: 100%[**************************************************->]0.063  train_acc:1.000  epoch71 test acc:0.6956521739130435\n",
      "epoch: 72 train loss: 100%[**************************************************->]0.107  train_acc:1.000  epoch72 test acc:0.7608695652173914\n",
      "epoch: 73 train loss: 100%[**************************************************->]0.046  train_acc:1.000  epoch73 test acc:0.8586956521739131\n",
      "epoch: 74 train loss: 100%[**************************************************->]0.081  train_acc:1.000  epoch74 test acc:0.8804347826086957\n",
      "epoch: 75 train loss: 100%[**************************************************->]0.067  train_acc:1.000  epoch75 test acc:0.9239130434782609\n",
      "epoch: 76 train loss: 100%[**************************************************->]0.037  train_acc:1.000  epoch76 test acc:0.8913043478260869\n",
      "epoch: 77 train loss: 100%[**************************************************->]0.170  train_acc:0.917  epoch77 test acc:0.9239130434782609\n",
      "epoch: 78 train loss: 100%[**************************************************->]0.242  train_acc:0.917  epoch78 test acc:0.8369565217391305\n",
      "epoch: 79 train loss: 100%[**************************************************->]0.509  train_acc:0.833  epoch79 test acc:0.8913043478260869\n",
      "epoch: 80 train loss: 100%[**************************************************->]0.649  train_acc:0.750  epoch80 test acc:0.8260869565217391\n",
      "epoch: 81 train loss: 100%[**************************************************->]0.359  train_acc:0.750  epoch81 test acc:0.7934782608695652\n",
      "epoch: 82 train loss: 100%[**************************************************->]0.567  train_acc:0.583  epoch82 test acc:0.9239130434782609\n",
      "epoch: 83 train loss: 100%[**************************************************->]0.073  train_acc:1.000  epoch83 test acc:0.717391304347826\n",
      "epoch: 84 train loss: 100%[**************************************************->]0.321  train_acc:0.833  epoch84 test acc:0.9021739130434783\n",
      "epoch: 85 train loss: 100%[**************************************************->]0.100  train_acc:1.000  epoch85 test acc:0.9021739130434783\n",
      "epoch: 86 train loss: 100%[**************************************************->]0.018  train_acc:1.000  epoch86 test acc:0.7934782608695652\n",
      "epoch: 87 train loss: 100%[**************************************************->]0.277  train_acc:0.917  epoch87 test acc:0.8260869565217391\n",
      "epoch: 88 train loss: 100%[**************************************************->]0.052  train_acc:1.000  epoch88 test acc:0.9347826086956522\n",
      "epoch: 89 train loss: 100%[**************************************************->]0.641  train_acc:0.667  epoch89 test acc:0.391304347826087\n",
      "epoch: 90 train loss: 100%[**************************************************->]0.156  train_acc:0.917  epoch90 test acc:0.9130434782608695\n",
      "epoch: 91 train loss: 100%[**************************************************->]0.191  train_acc:0.917  epoch91 test acc:0.8913043478260869\n",
      "epoch: 92 train loss: 100%[**************************************************->]0.699  train_acc:0.667  epoch92 test acc:0.7391304347826086\n",
      "epoch: 93 train loss: 100%[**************************************************->]0.162  train_acc:0.833  epoch93 test acc:0.8804347826086957\n",
      "epoch: 94 train loss: 100%[**************************************************->]0.103  train_acc:1.000  epoch94 test acc:0.9239130434782609\n",
      "epoch: 95 train loss: 100%[**************************************************->]0.112  train_acc:1.000  epoch95 test acc:0.8695652173913043\n",
      "epoch: 96 train loss: 100%[**************************************************->]0.582  train_acc:0.833  epoch96 test acc:0.8913043478260869\n",
      "epoch: 97 train loss: 100%[**************************************************->]0.459  train_acc:0.917  epoch97 test acc:0.6521739130434783\n",
      "epoch: 98 train loss: 100%[**************************************************->]0.631  train_acc:0.750  epoch98 test acc:0.7717391304347826\n",
      "epoch: 99 train loss: 100%[**************************************************->]0.129  train_acc:1.000  epoch99 test acc:0.8913043478260869\n",
      "epoch: 100 train loss: 100%[**************************************************->]0.521  train_acc:0.750  epoch100 test acc:0.9021739130434783\n",
      "Finish !\n",
      "Best epoch: 62  Best Acc: 0.9347826086956522\n"
     ]
    }
   ],
   "execution_count": 70
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
